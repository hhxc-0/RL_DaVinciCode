{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on `https://github.com/higgsfield-ai/higgsfield/rl/rl_adventure_2/3.ppo.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import FlattenObservation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import davinci_code_env_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from actor_critic import ActorCritic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Environments</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(render_mode=None):\n",
    "    env = gym.make(\n",
    "        \"DavinciCode-v2\",\n",
    "        render_mode=render_mode,\n",
    "        # max_episode_steps=100,\n",
    "        num_players=3,\n",
    "        # initial_player=0,\n",
    "        max_tile_num=12,\n",
    "        initial_tiles=4,\n",
    "    )\n",
    "    return env\n",
    "\n",
    "\n",
    "train_env = make_env()\n",
    "eval_env = make_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Evaluation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, corrects, invalids):\n",
    "    clear_output(True)\n",
    "\n",
    "    # Apply Gaussian filter\n",
    "    sigma = 4\n",
    "    rewards_smooth = gaussian_filter1d(rewards, sigma)\n",
    "    invalids_smooth = gaussian_filter1d(invalids, sigma)\n",
    "    corrects_smooth = gaussian_filter1d(corrects, sigma)\n",
    "\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(131)\n",
    "    plt.title(f\"frame {frame_idx}. reward: {rewards[-1]:.2f}\")\n",
    "    plt.plot(rewards, label=\"Reward\")\n",
    "    plt.plot(rewards_smooth, label=\"Smoothed Reward\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.title(f\"frame {frame_idx}. correct: {corrects[-1]:.2f}. invalid: {invalids[-1]:.2f}\")\n",
    "    plt.plot(corrects, label=\"Correct\")\n",
    "    plt.plot(corrects_smooth, label=\"Smoothed Correct\")\n",
    "    plt.plot(invalids, label=\"Invalid\")\n",
    "    plt.plot(invalids_smooth, label=\"Smoothed Invalid\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def eval_model(model, eval_env, print_info=False):\n",
    "    # with torch.no_grad():\n",
    "    state, _ = eval_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    correct_guess_count = 0\n",
    "    invalid_action_count = 0\n",
    "    frame_count = 0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, _ = model(state)\n",
    "        action = dist.sample().cpu().item()\n",
    "        if print_info:\n",
    "            print(f\"action: {action}\")\n",
    "        next_state, reward, terminated, truncated, info = eval_env.step(\n",
    "            action\n",
    "        )\n",
    "        if print_info:\n",
    "            print(f\"action_mask: {next_state[1]}\")\n",
    "            print(f\"reward: {reward}, terminated: {terminated}, truncated: {truncated}, info: {info}\")\n",
    "        done = np.logical_or(terminated, truncated)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if info[\"correct_guess\"]:\n",
    "            correct_guess_count += 1\n",
    "        if info[\"invalid_action\"]:\n",
    "            invalid_action_count += 1\n",
    "        frame_count += 1\n",
    "    return frame_count, total_reward, correct_guess_count, invalid_action_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>GAE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Proximal Policy Optimization Algorithm</h1>\n",
    "<h2><a href=\"https://arxiv.org/abs/1707.06347\">Arxiv</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids], log_probs[rand_ids], returns[\n",
    "            rand_ids\n",
    "        ], advantage[rand_ids]\n",
    "\n",
    "\n",
    "def ppo_update(\n",
    "    model,\n",
    "    optimizer,\n",
    "    ppo_epochs,\n",
    "    mini_batch_size,\n",
    "    states,\n",
    "    actions,\n",
    "    log_probs,\n",
    "    returns,\n",
    "    advantages,\n",
    "    clip_param=0.2,\n",
    "):\n",
    "    # torch.autograd.set_detect_anomaly(True)\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(\n",
    "            mini_batch_size, states, actions, log_probs, returns, advantages\n",
    "        ):\n",
    "            dist, value = model(state)\n",
    "            entropy = dist.entropy()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy.mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = train_env.observation_space.shape[0]\n",
    "num_outputs = train_env.action_space.n\n",
    "\n",
    "# Hyper params:\n",
    "shared_sizes = [512, 256, 128, 64]\n",
    "critic_sizes = [64, 64, 32, 32, 32]\n",
    "actor_sizes = [64, 64, 64, 128, 256]\n",
    "lr = 3e-4\n",
    "num_steps = 500\n",
    "mini_batch_size = 100\n",
    "ppo_epochs = 3\n",
    "threshold_reward = 200\n",
    "gae_gamma = 0.99\n",
    "gae_tau = 0.95\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs, shared_sizes, critic_sizes, actor_sizes).to(device)\n",
    "# model = torch.load(\"./ppo_model_saves/ppo_model_final.pth\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_frames = 1000000\n",
    "max_frames = np.inf\n",
    "frame_count = 0\n",
    "test_rewards = []\n",
    "corrects = []\n",
    "invalids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = train_env.reset()\n",
    "early_stop = False\n",
    "\n",
    "while frame_count < max_frames and not early_stop:\n",
    "\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    masks = []\n",
    "    entropy = 0\n",
    "\n",
    "    for step_num in range(num_steps):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        with torch.no_grad():\n",
    "            dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        next_state, reward, terminated, truncated, _ = train_env.step(\n",
    "            action.cpu().numpy()\n",
    "        )\n",
    "        done = np.logical_or(terminated, truncated)\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy()\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.tensor(reward, dtype=torch.float32).to(device))\n",
    "        masks.append(torch.tensor(1 - done).to(device))\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "\n",
    "        state = next_state\n",
    "        frame_count += 1\n",
    "\n",
    "        if done:\n",
    "            state, _ = train_env.reset()\n",
    "\n",
    "        if frame_count % 5000 == 0:\n",
    "            eval_results = np.array([eval_model(model, eval_env) for _ in range(10)])\n",
    "            test_total_frames = np.sum(eval_results[:, 0])\n",
    "            test_reward = np.mean(eval_results[:, 1])\n",
    "            correct_guess_rate = np.sum(eval_results[:, 2]) / test_total_frames\n",
    "            invalid_action_rate = np.sum(eval_results[:, 3]) / test_total_frames\n",
    "            test_rewards.append(test_reward)\n",
    "            corrects.append(correct_guess_rate)\n",
    "            invalids.append(invalid_action_rate)\n",
    "            plot(frame_count, test_rewards, corrects, invalids)\n",
    "\n",
    "            if test_reward > threshold_reward:\n",
    "                early_stop = True\n",
    "\n",
    "        if frame_count % 50000 == 0:\n",
    "            eval_results = np.array([eval_results for _ in range(30)])\n",
    "            test_total_frames = np.sum(eval_results[:, 0])\n",
    "            test_reward = np.mean(eval_results[:, 1])\n",
    "            correct_guess_rate = np.sum(eval_results[:, 2]) / test_total_frames\n",
    "            invalid_action_rate = np.sum(eval_results[:, 3]) / test_total_frames\n",
    "            torch.save(\n",
    "                model,\n",
    "                f\"./ppo_model_saves/ppo_model_frame{frame_count}_reward{test_reward:.2f}_correct{correct_guess_rate:.2f}_invalid{invalid_action_rate:.2f}.pth\",\n",
    "            )\n",
    "\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values, gae_gamma, gae_tau)\n",
    "\n",
    "    returns = torch.cat(returns).detach()\n",
    "    log_probs = torch.stack(log_probs).detach().to(device)\n",
    "    values = torch.cat(values).detach()\n",
    "    states = torch.stack(states)\n",
    "    actions = torch.tensor(actions).detach().to(device)\n",
    "\n",
    "    advantage = returns - values\n",
    "\n",
    "    ppo_update(\n",
    "        model,\n",
    "        optimizer,\n",
    "        ppo_epochs,\n",
    "        mini_batch_size,\n",
    "        states,\n",
    "        actions,\n",
    "        log_probs,\n",
    "        returns,\n",
    "        advantage,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env.close()\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = make_env(render_mode=\"human\")\n",
    "model = torch.load(\"./ppo_model_saves/ppo_model_final.pth\")\n",
    "eval_results = eval_model(model, eval_env, True)\n",
    "print(\"Evaluation results:\")\n",
    "print(f\"frame_count: {eval_results[0]} | total_reward: {eval_results[1]} | correct_guess_count: {eval_results[2]} | invalid_action_count: {eval_results[3]}\")\n",
    "eval_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".RL_DaVinciCode_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
